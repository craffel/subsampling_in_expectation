{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byGFSmTLEnxB"
   },
   "source": [
    "# Training a Subsampling Mechanism in Expectation\n",
    "\n",
    "Consider a mechanism which, given a sequence of vectors $\\mathbf{s} = \\{s_0, s_1, ... , s_{T - 1}\\}$, produces a sequence of \"sampling probabilities\" $\\mathbf{e} = \\{e_0, e_1, ..., e_{T - 1}\\}, e_t \\in [0, 1]$ which denote the probability of including $s_t$ in the output sequence $\\mathbf{y} = \\{y_0, y_1, ..., y_{U - 1}\\}$. Producing $\\mathbf{y}$ from $\\mathbf{s}$ and $\\mathbf{e}$ is encapsulated by the following pseudo-code:\n",
    "```\n",
    "# Initialize y as an empty sequence\n",
    "y = []\n",
    "for t in {0, 1, ..., T - 1}:\n",
    "    # Draw a random number in [0, 1] and compare to e[t]\n",
    "    if rand() < e[t]:\n",
    "        # Add s[t] to y with probability e[t]\n",
    "        y.append(s[t])\n",
    "```\n",
    "We call this a \"subsampling mechanism\", because by construction, $U \\le T$, and each element of $y$ is drawn directly from $s$.  When including this mechanism inside of a larger neural network model, it's possible to backpropagate through it by differentiating with respect to the expected output.  To compute the expected output, we first compute $p(y_m = s_n)$, i.e. the probability that the $m$th element of $\\mathbf{y}$ is the $n$th element of $\\mathbf{s}$, as follows:\n",
    "$$\n",
    "p(y_m = s_n) = \\begin{cases}\n",
    "0, n < m\\\\\n",
    "e_n\\prod_{i = 0}^{n - 1}(1 - e_i), m = 0\\\\\n",
    "e_n\\left(\\sum_{j = 0}^{n - 1}p(y_{m - 1} = s_j)\\prod_{i = j + 1}^{n - 1}(1 - e_i)\\right), \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "We can then compute the expected value of $y_m$ simply by computing $\\sum_n s_n p(y_m = s_n)$.  Further details are available in [1].  This notebook contains an example implementation of this approach in TensorFlow and Numpy for illustration purposes.\n",
    "\n",
    "[1] \"_Training a Subsampling Mechanism in Expectation_\", Colin Raffel & Dieterich Lawson.  Submitted to ICLR 2017, workshop track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1833,
     "status": "ok",
     "timestamp": 1487353445361,
     "user": {
      "displayName": "Colin Raffel",
      "photoUrl": "//lh4.googleusercontent.com/-6j_Ff9G1k5M/AAAAAAAAAAI/AAAAAAAAABA/_v4Bib1bJzk/s50-c-k-no/photo.jpg",
      "userId": "115298610791865473146"
     },
     "user_tz": 480
    },
    "id": "FjxevLpqDvpZ",
    "outputId": "546a2185-fce5-4feb-f558-15abacd117d3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "128X2yyrIpsq"
   },
   "source": [
    "### TensorFlow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1487353446117,
     "user": {
      "displayName": "Colin Raffel",
      "photoUrl": "//lh4.googleusercontent.com/-6j_Ff9G1k5M/AAAAAAAAAAI/AAAAAAAAABA/_v4Bib1bJzk/s50-c-k-no/photo.jpg",
      "userId": "115298610791865473146"
     },
     "user_tz": 480
    },
    "id": "L22LB5aOEmv3",
    "outputId": "d9fafd6d-93cb-41fc-d36f-39a6644739b7"
   },
   "outputs": [],
   "source": [
    "def safe_cumprod(x, **kwargs):\n",
    "    \"\"\"Computes cumprod in logspace using cumsum to avoid numerical issues.\"\"\"\n",
    "    return tf.exp(tf.cumsum(tf.log(tf.clip_by_value(x, 1e-10, 1)), **kwargs))\n",
    "\n",
    "def output_probabilities(p_emit):\n",
    "    \"\"\"Given emission probabilities p_emit, compute p(y_m = s_n).\n",
    "\n",
    "    Given the probability of emitting, compute the probability of the output\n",
    "    being each state.  E.g., given p_emit whose entries correspond to the\n",
    "    probability of emitting a token at time t when producing an output sequence y,\n",
    "    compute the matrix p(y_m = s_n).\n",
    "\n",
    "    Args:\n",
    "        p_emit: The probability of emitting at each time, shape (n_batch, n_seq)\n",
    "\n",
    "    Returns:\n",
    "        A Tensor of shape (n_batch, n_seq, n_seq), where the entry (b, m, n)\n",
    "        corresponds to p(y_m, s_n) for batch b.\n",
    "    \"\"\"\n",
    "    # Retrieve the number of sequence steps\n",
    "    n_batch = tf.shape(p_emit)[0]\n",
    "    n_seq = tf.shape(p_emit)[1]\n",
    "\n",
    "    def build_prod_1_m_h_jp1_nm1(_, j):\n",
    "        r\"\"\"Computes cumprod(1 - p_emit[j + 1:]) for each batch.\"\"\"\n",
    "        # Compute the cumulative product \\prod_{i = j + 1}^{n - 1}(1 - h_i) for this\n",
    "        # value of j.\n",
    "        # exclusive=True means include a [1] at the beginning\n",
    "        prod_1_m_h = safe_cumprod(1. - p_emit[:, j + 1:], axis=1, exclusive=True)\n",
    "        # Pad with zeros to make it length-N\n",
    "        return tf.concat([tf.zeros((n_batch, j + 1)), prod_1_m_h], 1)\n",
    "    # Build the prod_1_m_h_jp1_nm1 using scan\n",
    "    prod_1_m_h_jp1_nm1 = tf.scan(\n",
    "        build_prod_1_m_h_jp1_nm1, tf.range(n_seq), tf.ones((n_batch, n_seq,)))\n",
    "    # Reshape to (n_batch, n_seq, n_seq)\n",
    "    prod_1_m_h_jp1_nm1 = tf.transpose(prod_1_m_h_jp1_nm1, [1, 0, 2])\n",
    "\n",
    "    def build_p_y_m_eq_s_n(previous_outputs, _):\n",
    "        \"\"\"Function to build rows of the matrix of values of p(y_m = s_n).\"\"\"\n",
    "        # This replicates the following:\n",
    "        # for n in range(p_emit.shape[0]):\n",
    "        #   p_y_m_eq_s_n[m, n] = p_emit[n]*np.sum(\n",
    "        #       p_y_m_eq_s_n[m - 1]*prod_1_m_h_jp1_nm1[:, n])\n",
    "        p_y_mm1_eq_s_n = previous_outputs[1]\n",
    "        p_y_m_eq_s_n = (p_emit * tf.matmul(\n",
    "            tf.reshape(p_y_mm1_eq_s_n, (n_batch, 1, n_seq)),\n",
    "            prod_1_m_h_jp1_nm1)[:, 0])\n",
    "        # We need to return the new row of the matrix (p_y_m_eq_s_n) as well as the\n",
    "        # old row (p_y_mm1_eq_s_n) because we want the output of scan to include\n",
    "        # p_y_0_eq_s_n\n",
    "        return p_y_mm1_eq_s_n, p_y_m_eq_s_n\n",
    "\n",
    "    # Compute first row of matrix, to pass as initial value to scan\n",
    "    p_y_0_eq_s_n = p_emit*safe_cumprod(1 - p_emit, axis=1, exclusive=True)\n",
    "    # Use scan to construct the p_y_m_eq_s_n matrix\n",
    "    p_y_m_eq_s_n = tf.scan(build_p_y_m_eq_s_n, tf.range(n_seq),\n",
    "                           (p_y_0_eq_s_n, p_y_0_eq_s_n))[0]\n",
    "    # Reshape to (n_batch, n_seq, n_seq)\n",
    "    p_y_m_eq_s_n = tf.transpose(p_y_m_eq_s_n, [1, 0, 2])\n",
    "    return p_y_m_eq_s_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7ZKIxvLIuxW"
   },
   "source": [
    "### Numpy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1487353448638,
     "user": {
      "displayName": "Colin Raffel",
      "photoUrl": "//lh4.googleusercontent.com/-6j_Ff9G1k5M/AAAAAAAAAAI/AAAAAAAAABA/_v4Bib1bJzk/s50-c-k-no/photo.jpg",
      "userId": "115298610791865473146"
     },
     "user_tz": 480
    },
    "id": "hzFANiuEIuX8",
    "outputId": "c182061c-8b14-4897-f64f-983835d23b75"
   },
   "outputs": [],
   "source": [
    "def output_probabilities_explicit(p_emit):\n",
    "    \"\"\"Explicitly compute p(y_m = s_n) using nested for loops.\"\"\"\n",
    "    p_y_m_eq_s_n = np.zeros((p_emit.shape[0], p_emit.shape[0]))\n",
    "    for m in range(p_emit.shape[0]):\n",
    "        for n in range(p_emit.shape[0]):\n",
    "            # p(y_m = s_n) = 0 when n < m because in order for the output sequence\n",
    "            # to be of length m, at least m - 1 symbols must already have been\n",
    "            # emitted.\n",
    "            if n < m:\n",
    "                p_y_m_eq_s_n[m, n] = 0.\n",
    "            # The probability that the first output element y_0 is a given element\n",
    "            # in the sequence s_n is the probability that none of s_0, ...,\n",
    "            # s_{n-1} were emitted multiplied by the probability of emitting s_n.\n",
    "            elif m == 0:\n",
    "                p_y_m_eq_s_n[m, n] = p_emit[n]*np.prod(1 - p_emit[:n])\n",
    "            # In order for y_m = s_n in general, we must have:\n",
    "            # y_{m - 1} = s_j must be one of the states before s_n\n",
    "            # None of s_j, ..., s_{n - 1} may be emitted at time m\n",
    "            # s_n is emitted at time n.\n",
    "            else:\n",
    "                for j in range(0, n):\n",
    "                    p_y_m_eq_s_n[m, n] += (p_y_m_eq_s_n[m - 1, j] *\n",
    "                                           np.prod(1 - p_emit[j + 1:n]))\n",
    "                p_y_m_eq_s_n[m, n] *= p_emit[n]\n",
    "    return p_y_m_eq_s_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ztw__gZI119"
   },
   "source": [
    "### Test that they're the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2589,
     "status": "ok",
     "timestamp": 1487353453922,
     "user": {
      "displayName": "Colin Raffel",
      "photoUrl": "//lh4.googleusercontent.com/-6j_Ff9G1k5M/AAAAAAAAAAI/AAAAAAAAABA/_v4Bib1bJzk/s50-c-k-no/photo.jpg",
      "userId": "115298610791865473146"
     },
     "user_tz": 480
    },
    "id": "hHdv3-iBI1YS",
    "outputId": "b7aa0789-03ed-4b57-b285-58ee6159e401"
   },
   "outputs": [],
   "source": [
    "# Create some test input emission probabilitiy sequences\n",
    "test_input = np.random.uniform(size=(10, 20))\n",
    "\n",
    "# Build tensorflow graph to compute tensorflow version\n",
    "with tf.Session() as sess:\n",
    "    p_emit = tf.placeholder(tf.float32, [None, None])\n",
    "    # Compute p(y_m = s_n) using tensorflow utility function\n",
    "    p_y_m_eq_s_n = output_probabilities(p_emit)\n",
    "    tensorflow_output = sess.run(p_y_m_eq_s_n, {p_emit: test_input})\n",
    "\n",
    "assert np.allclose(\n",
    "    tensorflow_output,\n",
    "    [output_probabilities_explicit(batch) for batch in test_input])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Subsampling in Expectation.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
